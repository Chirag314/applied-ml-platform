# ğŸ§  Applied ML Platform â€” End-to-End Training, Evaluation & Deployment

A **production-grade applied machine learning platform** that demonstrates how real ML systems are built, evaluated, versioned, and served â€” beyond notebooks and one-off experiments.

This project focuses on **ML system ownership**:
- Reproducible training pipelines
- Leakage-safe evaluation
- Versioned artifacts
- API-based inference
- Runtime model management

---

## ğŸ”– Badges

<p align="center">
  <img src="https://img.shields.io/badge/python-3.11-blue" />
  <img src="https://img.shields.io/badge/fastapi-serving-brightgreen" />
  <img src="https://img.shields.io/badge/ml-tabular%20%7C%20time--series-orange" />
  <img src="https://img.shields.io/badge/tests-passing-success" />
</p>

---

## ğŸŒŸ Features

  âœ” Config-driven, reproducible ML training  
  âœ” Tabular ML (classification / regression)  
  âœ” Time-series forecasting with walk-forward validation  
  âœ” Versioned model artifacts per run  
  âœ” FastAPI inference service  
  âœ” Runtime model reload without redeploy  
  âœ” Backtest plots & metrics saved automatically  

---

## ğŸ— High-Level Architecture

```text
Configs (YAML)
      â”‚
      â–¼
Training Pipelines
(tabular | time-series)
      â”‚
      â–¼
Model Registry
(artifacts, metadata,
 metrics, backtests)
      â”‚
      â–¼
Inference API (FastAPI)
 /predict
 /predict-timeseries
 /reload-model

```
---

## ğŸ“¦ Project Structure

applied-ml-platform/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/            # Data generation & feature engineering
â”‚   â”œâ”€â”€ training/        # Training pipelines per modality
â”‚   â”œâ”€â”€ models/          # Model registry & artifact handling
â”‚   â””â”€â”€ utils/           # Config & metrics helpers
â”‚
â”œâ”€â”€ service/
â”‚   â””â”€â”€ app.py           # FastAPI inference service
â”‚
â”œâ”€â”€ configs/             # YAML experiment configs
â”œâ”€â”€ artifacts/           # Versioned model runs
â”œâ”€â”€ tests/               # Smoke & integration tests
â””â”€â”€ README.md

---
## ğŸ§  Supported ML Modalities
### 1ï¸âƒ£ Tabular Machine Learning

    - Classification / regression

    - Feature shape validation

    - Probability outputs

    - REST-based inference

---

## 2ï¸âƒ£ Time-Series Forecasting

    - Walk-forward (leakage-safe) cross-validation

    - Lag & rolling-window features

    - Recursive multi-step forecasting

    - Offline backtests saved per run

---

## ğŸ“Š Time-Series Backtest (Example)

![Time-series Backtest](artifacts/ts_baseline_v1/backtest.png)

Automatically generated during training  
(last fold, walk-forward validation):

Each run produces:
- `model.pkl`
- `metadata.json`
- `backtest.csv`
- `backtest.png`

---

## ğŸš€ Training

### Tabular model training

python -m src.training.train --config configs/example_tabular.yaml

### Time-series model training

python -m src.training.train_timeseries --config configs/example_timeseries.yaml

### Artifacts are stored under:

artifacts/<run_name>/

### The active production model is controlled via:

artifacts/latest.txt

---

## ğŸŒ Inference Service
### Start the API

uvicorn service.app:app --reload --port 8000

Endpoint	        Method	Description

/health	            GET	    Service & model health
/model-info	        GET   	Active model metadata
/predict	        POST	Tabular inference
/predict-timeseries	POST	Multi-step time-series forecast
/admin/reload-model	POST	Reload active model

### Swagger UI:
http://localhost:8000/docs

---

## ğŸ”„ Model Lifecycle

1. Train model via config

2. Evaluate with offline metrics & backtests

3. Save versioned artifacts

4. Promote model via latest.txt

5. Serve via API

6. Reload model without restarting service

7. This mirrors real production ML workflows.

---
## ğŸ›£ Roadmap

Multimodal ensembles (tabular + time-series)

Drift detection & monitoring

Feature store integration

Batch inference pipelines

Model cards per run

CI validation of training configs

---

### â­ If you find this project useful, consider starring the repository.


